{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import pickle\n",
    "import time, datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import gc\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import jieba, pdb\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import json\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#from DataProcessing_test import *\n",
    "\n",
    "# import matplotlib.pylab as plt\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import numpy as np\n",
    "from gplearn import genetic\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tags, df_cat,df_add,df_ts, df_cap,df_imfeat, df_label,df_user = PreProcessing_test()\n",
    "\n",
    "\n",
    "# df_tags.to_pickle('train/tags_processed_all.pkl')\n",
    "# df_cat.to_pickle('train/cat_processed_all.pkl')\n",
    "# df_add.to_pickle('train/add_processed_all.pkl')\n",
    "# df_ts.to_pickle('train/ts_processed_all.pkl')\n",
    "# df_cap.to_pickle('train/cap_processed_all.pkl')\n",
    "# df_imfeat.to_pickle('train/imfeat_processed_all.pkl')\n",
    "# df_label.to_pickle('train/label_processed_all.pkl')\n",
    "#df_user.to_pickle('train/user_processed_all.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags = pd.read_pickle('train/tags_processed_all.pkl')\n",
    "df_cat  = pd.read_pickle('train/cat_processed_all.pkl')\n",
    "df_add  = pd.read_pickle('train/add_processed_all.pkl')\n",
    "df_ts   = pd.read_pickle('train/ts_processed_all.pkl')\n",
    "df_cap  = pd.read_pickle('train/cap_processed_all.pkl')\n",
    "df_imfeat  = pd.read_pickle('train/imfeat_processed_all.pkl')\n",
    "df_label= pd.read_pickle('train/label_processed_all.pkl')\n",
    "#df_user = pd.read_pickle('train/user_two_processed_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 486194 training with 305613 labels\n",
      "Got 486194 training with 305613 labels\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Got %d training with %d labels\" % (len(df_cat), len(df_label)))\n",
    "\n",
    "df_tags = df_tags.drop(['Unnamed: 0'], axis=1)\n",
    "df_imfeat = df_imfeat.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "df_all = df_cat.merge(df_tags, on=['Uid', 'Pid'])\n",
    "df_all = df_ts.merge(df_all, on=['Uid', 'Pid'])\n",
    "df_all = df_add.merge(df_all, on=['Uid', 'Pid'])\n",
    "df_all = df_cap.merge(df_all, on=['Uid', 'Pid'])\n",
    "#df_all = df_user.merge(df_all, on=['Uid', 'Pid'])\n",
    "ss_len = len(df_all.columns)\n",
    "\n",
    "df_all = df_all.merge(df_imfeat, on=['Uid', 'Pid'], how='outer')\n",
    "# df_all=  df_all.drop(['Uid', 'Pid'], axis=1)\n",
    "df_train = df_all.iloc[0:305613]\n",
    "df_test  = df_all.iloc[305613:]\n",
    "\n",
    "# train/test data\n",
    "print(\"Got %d training with %d labels\" % (len(df_all), len(df_label)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rn\n",
    "rn.seed(6666)\n",
    "df_uid = df_cat.iloc[0:305613].drop_duplicates('Uid')\n",
    "uids=df_uid.values\n",
    "len1=len(df_uid)\n",
    "train_list=[]\n",
    "val_list=[]\n",
    "idx=rn.sample(range(len1), len1)\n",
    "train_len=int(len1*0.7)\n",
    "train_uid=list(df_uid['Uid'].values[idx[:train_len]])\n",
    "val_uid=list(df_uid['Uid'].values[idx[train_len:]])\n",
    "\n",
    "len1=305613\n",
    "for i in range(len1):\n",
    "    if df_cat['Uid'].values[i] in val_uid:\n",
    "        val_list.append(i)\n",
    "    else:\n",
    "        train_list.append(i)\n",
    "\n",
    "val_list=np.array(val_list)\n",
    "train_list=np.array(train_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imf=df_all.values[:,ss_len:]\n",
    "from sklearn.decomposition import PCA\n",
    "model=PCA( n_components=100)\n",
    "model.fit(imf)\n",
    "imf=model.transform(imf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta1, roberta2 = np.load('roberta.feat.npy')\n",
    "model=PCA( n_components=400)\n",
    "model.fit(roberta1)\n",
    "roberta1=model.transform(roberta1)\n",
    "\n",
    "model=PCA( n_components=400)\n",
    "model.fit(roberta2)\n",
    "roberta2=model.transform(roberta2)\n",
    "print(roberta2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = LinearDiscriminantAnalysis()\n",
    "# imf=df_all.values[train_list,ss_len:]\n",
    "# YY = df_label.values[train_list]\n",
    "# imy = np.asarray(np.reshape(YY, [-1]),np.int)\n",
    "# clf.fit(imf, imy)  \n",
    "# imf=df_all.values[:,ss_len:]\n",
    "# imf=clf.transform(imf)\n",
    "# X= np.concatenate((X, imf[0:305613,:]),axis=1)\n",
    "# X2= np.concatenate((X2, imf[305613:,:]),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.values\n",
    "\n",
    "X=X[:,:ss_len]\n",
    "X= np.concatenate((X, imf[0:305613,:]),axis=1)\n",
    "X= np.concatenate((X, roberta2[0:305613,:]),axis=1)\n",
    "X= np.concatenate((X, roberta1[0:305613,:]),axis=1)\n",
    "print('X shape:', X.shape)\n",
    "Y = df_label.values\n",
    "Y = np.reshape(Y,[-1])\n",
    "Y2=np.asarray(Y,np.int)\n",
    "\n",
    "\n",
    "X2 = df_test.values\n",
    "\n",
    "X2=X2[:,:ss_len]\n",
    "X2= np.concatenate((X2, imf[305613:,:]),axis=1)\n",
    "X2= np.concatenate((X2, roberta2[305613:,:]),axis=1)\n",
    "X2= np.concatenate((X2, roberta1[305613:,:]),axis=1)\n",
    "print('Test data shape:', X2.shape)\n",
    "del roberta1, roberta2, imf\n",
    "\n",
    "oof_preds = np.zeros(X.shape[0])\n",
    "feature_importance_df = pd.DataFrame()\n",
    "t_value = np.zeros(X2.shape[0])\n",
    "t_cnt = 0\n",
    "\n",
    "train_x, train_y = X[train_list,:], Y[train_list]\n",
    "\n",
    "\n",
    "# train_x, train_y = X, Y\n",
    "valid_x, valid_y = X[val_list,:], Y[val_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Index:\",train_list,\",Val Index:\",val_list)\n",
    "print(len(train_list),len(val_list))\n",
    "\n",
    "## dart\n",
    "params = {\n",
    "'nthread': 16, 'boosting_type': 'dart', 'objective': 'regression', 'metric': ['mae', 'huber'], \n",
    "    'learning_rate': 0.02, 'num_leaves':127,\n",
    "    'max_depth': 8, 'subsample': 0.8, 'feature_fraction': 0.7, \n",
    "    'min_split_gain': 0.09, 'min_child_weight': 9.5,\n",
    "    'drop_rate':0.7, 'skip_drop':0.7, 'max_drop': 140, 'uniform_drop':False, \n",
    "    'xgboost_dart_mode':True, 'drop_seed':1  }\n",
    "\n",
    "\n",
    "dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "dval = lgb.Dataset(valid_x, label=valid_y, reference=dtrain)\n",
    "bst = lgb.train(params, dtrain, num_boost_round=30000, valid_sets=[dval], early_stopping_rounds=20000, verbose_eval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in range(0,30000,500):\n",
    "\n",
    "    tmp_valid = bst.predict(valid_x, num_iteration=it)\n",
    "    print('%d-iter, Rank corr : %.6f' % (it, stats.spearmanr(valid_y, tmp_valid)[0]), end=', ')\n",
    "    print('MAE : %.6f, MSE : %.6f' % (mae(valid_y, tmp_valid), mse(valid_y, tmp_valid)))\n",
    "\n",
    "del bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(X, label=Y)\n",
    "bst = lgb.train(params, dtrain, num_boost_round=30000, verbose_eval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_value = bst.predict(X2, num_iteration=24000)\n",
    "\n",
    "\n",
    "order = t_value.argsort()\n",
    "ranks = order.argsort()\n",
    "ranks = max(ranks) - ranks+1\n",
    "df_cat = pd.read_json(\"test/test_category.json\")\n",
    "\n",
    "y=pd.DataFrame(t_value)\n",
    "y.columns=['y']\n",
    "df_res = df_cat.join(y)\n",
    "df_res=df_res[['Pid','y']]\n",
    "df_res=df_res.astype({'Pid':int})\n",
    "df_res.columns=['post_id','popularity_score']\n",
    "res_key = pd.DataFrame(['result' for i in range(len(df_res))])\n",
    "df_res = df_res.join(res_key)\n",
    "\n",
    "df_res.columns=['post_id','popularity_score', 'result']\n",
    "df_res['post_id'] = df_res['post_id'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = df_res.groupby(['result'], as_index=False).apply(lambda x: x[['post_id','popularity_score']].to_dict('r')).reset_index().rename(columns={0:'result'}).to_json(orient='records')\n",
    "j = j.replace('\"index\":0', '\"version\":1.1')\n",
    "j = j[1:-1]\n",
    "with open('submission6.json','w') as fp:\n",
    "    fp.write(j)\n",
    "\n",
    "print('Good job!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
